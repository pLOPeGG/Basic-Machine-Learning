\documentclass[a4paper, 10pt]{article}

\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\setlength{\marginparwidth}{2cm}

\usepackage{comment}
\usepackage{todonotes}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{enumitem}

\usepackage{xcolor}
\usepackage{graphicx}
\graphicspath{ {./img/} }

\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
    language=Python,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\title{Homework Assignment N°3}
\author{BML36\\Thibault Douzon\\Rajavarman Mathivanan}
\date{September 18th, 2018}

\begin{document}
\maketitle

\pagebreak

\tableofcontents

\pagebreak

\section{Exercise 1: K-CV, under \& over-fitting}

\subsection{Part a}
To show what are under and over-fitting, we will use polynomial 
regression with different degrees on sample data. We generate 
our data points using a degree 3 polyomial and adding gaussian noise. Here is the dataset:
\\
\includegraphics{ex1a_curve}
\\
Our model minimizes the sum of the squared error. Because our underlying function is a degree 3
polynomial, the best model we computed is the following, also degree 3:
\\
\includegraphics{ex1a_good}
\\
We can see that the green line follows accordingly our dataset, without trying to go through
every points. We can expect this model to generalize well.
\begin{itemize}[label=$\square$]
    \item Under-fitting comes when the chosen model is not complex enough
    to represent the learning data and the underlying function. 
    \\
    An under-fit model suffer from high \emph{bias} and high error on learning data.    
    We would say from such a model that it does not fit enough to the data.
    \\
    An example is our degree 0 polynomial: an horizontal line can't describe 
    correctly our underlying degree 3 polynomial:
    \\
    \includegraphics{ex1a_under}
    \\
    The x and y axis represent the $x$ and $y$ coordinates for our points such that
    $f(x) \approx y$.
    \\
    We can see on the previous plot that the green line is close to points in the 
    middle but makes a non negligeable error on extreme points.
    \\
    When controlling the learning with a test dataset, we know we are under-fitting when
    both the error on the training and the test data is high: our model does not describe 
    correctly the learning data and can't generalize to other data.
    
    \item Over-fitting comes when to model sticks too much to the learning data. In some sense, 
    our model tries to interpret the noise present in the learning data as if it were significant.
    \\
    An over-fit model suffer from high \emph{variance} and high error on testing data.
    \\
    An example of over-fit model is our degree 20 polynomial model:
    \\
    \includegraphics{ex1a_over}
    \\
    As before, this plot shows the learning data and the curve of the model.
    We can see the curve tries to pass through every points even those remote from the underlying
    curve (which suffer from high noise).
    \\ 
    Another sign of over-fitting is very high value for parameters of the model (absolute value). 
    This symptom is why we can use L1 or L2 regularization to prevent us from over-fitting.
\end{itemize}
To better see both phenomena in one figure, we can plot the error in term of the degree of our model.
If we do so for both training and testing dataset, we get the following plot:
\\
\includegraphics{ex1a_error}
\\
On this figure we can see that the error on learning data is globally always decreasing (very steeply at the beginning, then slowly)
when the complexity of the model increases. But the curve of the error on the testing data does not follows the same pattern: it first 
decreases up to some degree and then increases again. 
\\
We can divide the previous figure in two areas, one represents under-fitting and the other over-fitting. And between the 
two lays the models that may generalyze well:
\\
\includegraphics{ex1a_area}
\\
As we said before, in the red area both curves decrease steeply: this is the under-fitting zone.
In the blue one, error on learning data continue decreasing but slowly while error on testing data increases:
this is the over-fitting zone.

\subsection{Part b}
It is not a good idea to simply divide our dataset in training/testing and
test each model on the test set to select the best model because it could result in over-fitting
and model the noise in the test set instead of the underlying model. Especially if we 
provide more data to the learning set than to the test set: the smaller the test set,
the more noisy is our testing, and the more probable a model can fall in over-fitting.
\\
Indeed, nothing tells us the the best model on the test set will generalize well and isn't 
over-fitting the test sample also.
\\
To prevent us from over-fitting the data that aren't in the learning set, we need
validation set, to finally evaluate the performance of our model on brand new data.
This validation set will allow us to chose the best hyperparameters for our models and chose the
best model between the trained ones.
\\
The terms "validation" and "test" are very ambiguous and often interchanged and some
call the validation set the holdout set.

WEIRD test / validation
\\
livre ->    validation = à chaque étape, si l'E augmente -> stop training
\\          test = à la fin pour choisir le meilleur
\\
diapos ->   validation = ???
\\          test = à chaque étape si l'E augmente -> stop

\subsection{Part c}
We use K-fold cross-validation because we want to use as much data as possible for training.
But keeping away test and validations sets from our training set. K-CV allows us to use the whole
dataset for training and still keep an eye on the over-fitting whereas data in the validation set will never
be used for learning.
\\
Pros: 
\begin{itemize}
    \item The whole dataset can be used for training. Very useful when we don't have many data.
    \item Can be used to determine the hyperparameters of our models: train all the
    different models with different parameters K times each and pick the best at the end
    (the one with the minimal mean of the error on the evaluation part remaining)
    \item It provides an accurate estimation of the performance of the model
\end{itemize}
Cons:
\begin{itemize}
    \item The learning time is increased by a factor K: if we split our dataset in 10 parts, we 
    will need 10 different trainings and thus it will last 10 times more to train.
    \item The number of folds we make is another parameter: when the dataset is small we can split in
    as many sets as the number of data, but it's too computer intensive when the dataset gets large.
\end{itemize}
\section{Exercise 2: ROC curve}
\subsection{Part a}
We can represent this situation with a table where a column represents what the model predicts (positive or negative)
and the rows represents the reality of the data. In the heading, the probability that the inequation is true is between the parenthesis
\\
\begin{center}
\begin{tabular}{ |c|c|c| }
    \hline
    \  & $x<\theta$ $\rightarrow$ ($\theta$) & $x>\theta$ $\rightarrow$ ($1-\theta$) \\
    \hline
    P & $\text{TP} = \theta \text{P}$ & $\text{FN} = (1-\theta) \text{P}$ \\
    N & $\text{FP} = \theta \text{N}$ & $\text{TN} = (1-\theta) \text{N}$ \\
    \hline
\end{tabular}
\end{center}
We deduce the following from the previous table:
$$
\text{TP}_{\text{rate}} = \frac{\text{TP}}{\text{TP}+\text{FN}} = \frac{\theta \text{P}}{\theta \text{P} +(1-\theta)\text{P}} = \theta
$$
$$
\text{FP}_{\text{rate}} = \frac{\text{FN}}{\text{FP}+\text{TN}} = \frac{\theta \text{N}}{\theta \text{N} + (1-\theta)\text{N}} = \theta
$$
For a random classifier such as this one, TP and FP rate do not depend
on P and N but only on $\theta$.

\subsection{Part b}
From the previous question, we can draw the ROC curve of the classifier.
Because we have the following identity $\text{TP}_\text{rate} = \text{FP}_\text{rate} = \theta$,
when we plot the $\text{TP}_\text{rate}$ in terms of the $\text{FP}_\text{rate}$ we get
the identity function (because both quantities are always equal to each other).
\\
\includegraphics{ex2b}
\\
This is a well known result: a random classifier has a ROC curve that
splits the ROC space in two.

\subsection{Part c}
Now it is easy to compute the AUC, this ROC curve is a line
following the diagonal of the base square, thus the are under the curve is
half the area of the square.
$$
\text{AUC} = \frac{1}{2}
$$
We could get to the same result using calculus: the underlying function
is $f(x) = x$.
\\
Calculus gives us the formula for the AUC:
$$
\text{AUC} = \int_0^1 f(x) dx
$$
$$
\text{AUC} = \int_0^1 x dx = \left[\frac{x^2}{2} \right]_0^1 
$$
$$
\text{AUC} = \frac{1}{2}
$$

\subsection{Part d}
The equal error rate points lay on the $x=y$ curve, exactly like our random classifier.
This ROC curve seems to only have two fixed points: $(0,0)$ and $(1,1)$
\end{document}

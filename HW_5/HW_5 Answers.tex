\documentclass[a4paper, 10pt]{article}

\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\setlength{\marginparwidth}{2cm}

\usepackage{comment}
\usepackage{todonotes}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\usepackage{enumitem}
\usepackage{array}
\setlength\extrarowheight{5pt}

\usepackage{xcolor}
\usepackage{graphicx}
\graphicspath{ {./img/} }

\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
    language=Python,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\title{Homework Assignment N°4}
\author{BML36\\Thibault Douzon\\Rajavarman Mathivanan}
\date{October 3rd, 2018}

\begin{document}
\maketitle

\pagebreak

\tableofcontents

\pagebreak
\section{Exercise 1: NN 1-D output}
\subsection{Part a}

\section{Exercise 2: NN 2-D output}
\subsection{Part a}
The neural network described in this exercise could be represented like this.
\begin{center}
\includegraphics[scale=0.5]{ex2_graph_export}
\end{center}
It is structured in 3 parts: the input layer first (denoted $x1$, $x2$), then the hidden layer
(denoted $h1$, $h2$ and $h3$) and then the output layer (denoted $o1$ and $o2$).
\\
Nodes with indice 0 represent the bias introducted into the model.
\\ 
Each arc carries the value of the weight (denoted $w_{k,j}^{(l)}$ where $l$ is the layer, $k$ is the destination node
and $j$ is the origin node which lies in the layer $l-1$).

To compute the output of the neural network, we need to propagate
through the network the values of the input. Hidden layer applies a sigmoïd activation function
and output neurons have a linear activation function (identity function).
\\
Thus we can first compute every $a_k^{(1)}$, the signal recieved by each node in the hidden layer.
It is easier to make this computation under matrix representation, let's introduce the input vector $X$ and
the weights of the first layer $W^{(1)}$:
$$
X = [a_i^{(0)}] = [x_i] =\begin{bmatrix}
    1\\
    1\\
    1
\end{bmatrix}
$$
$$
W^{(1)} = [w_{j,i}^{(1)}] =  \begin{bmatrix}
    1 & 0 & -1\\
    1 & -1 & 0\\
    -1 & -1 & 1
\end{bmatrix}
$$
The signal recieved by the hidden layer is given by the following formula:
$$
H_{j\ne0} = [a_j^{(1)}]_{j\ne0} = X^\top W^{(1)} = \begin{bmatrix}
    1\\
    -2\\
    0
\end{bmatrix}
$$
Before repeating the same procedure, we need to apply the activation function
to each recieved signal and then concatenate the bias.
\\
The hidden layer uses the sigmoïd function as activation:
$$
\sigma(H_{j\ne0}) = [\sigma(a_j{(1)})] = \begin{bmatrix}
    \sigma(1)\\
    \sigma(-2)\\
    \sigma(0)
\end{bmatrix}
\approx \begin{bmatrix}
        0.731\\
        0.119\\
        0.5
\end{bmatrix}
$$
Now we can concatenate the bias at the beginning with a fixed value of 1:
$$
H_{out} \approx \begin{bmatrix}
    1\\
    0.731\\
    0.119\\
    0.5
\end{bmatrix}
$$
The vector $H_{out}$ is the signed emitted by the hidden layer.
\\
We can now repeat the same procedure with the second layer with $H_{out}$ as input and 
use the weights of the output layer.
\\
The weights of the second layer are the following:
$$
W^{(2)} = [w_{k,j}^{(2)}] = \begin{bmatrix}
    1 & -1\\
    -1 & 1\\
    1 & -1\\
    -1 & 1
\end{bmatrix}
$$
And we can compute the signal recieved by the output layer:
$$
O = [a_k^{(2)}] = {H_{out}}^\top W^{(2)} \approx \begin{bmatrix}
    -0.112\\
    0.112
\end{bmatrix}
$$
This is the output of our neural network as the activation
fonction of the output neuron is the identity function.

\subsection{Part b}
The formula to compute $\delta_i^{(l)}$ is the following:
$$
\delta_i^{(l)} = \frac{\partial E_n}{\partial a_i^{(l)}}
$$
Where $l$ is the layer and $i$ is the index of the neuron.
\\
But to compute the value of this expression for different values of $l$ and $i$,
we need to know the expression of $E_n$ first. $E_n$ is the error of the network
on the data sample number $n$. It is defined as follows:
$$
E_n(w) = \frac{1}{2} \sum_{k=1}^D ( y_k(x_n,w)-t_{n,k})^2
$$
Where $N$ is the number of output neurons (in our case 2), $x_n$ the input data and 
$t_n$ the target related to $x_n$, $w$ the current weigths of the neural network and 
$y_k$ the function that computes the $k^\text{th}$ coordinate of the NN's prediction based on the input data.
\\
In our case, we can rewrite this expression to this:
$$
E_n(w) = \frac{1}{2} \sum_{k=1}^2( y_k(x_n, w)-t_{n,k})^2
$$
And because the activation function of the output layer is the identity function,
we also have the following equation:
$$
y_k(x_n,w) = h(a_k^{(2)}) = a_k^{(2)}
$$
The analytical expressions of $\delta_1^{(2)}$ and $\delta_2^{(2)}$ directly follows:
$$
\delta_i^{(2)} = \frac{\partial E_n}{\partial a_i^{(2)}} = \frac{1}{2}\frac{\partial  \sum_{k=1}^2( a_k^{(2)}-t_{n,k})^2}{\partial a_i^{(2)}} = a_i^{(2)} - t_{n,i}
$$
We finally get the following numerical values:
$$
\delta_1^{(2)} = a_1^{(2)} - t_{n,1} = -0.112 - 1 = -1.112
$$
$$
\delta_2^{(2)} = a_2^{(2)} - t_{n,2} = 0.112 - (-1) = 1.112
$$

\subsection{Part c}
The formula to update a weight is 

\end{document}
    
$$
\frac{\partial E}{\partial w_i}(w) = \frac{\partial \frac{1}{2}\sum_{n=1}^N (\sigma(w^\top x_n) - t_n)^2}{\partial w_i}
$$
$$
\frac{\partial E}{\partial w_i}(w) =\sum_{n=1}^N \frac{\partial \frac{1}{2} (\sigma(w^\top x_n) - t_n)^2}{\partial \sigma(w^\top x_n)-t_n} \cdot \frac{\partial \sigma(w^\top x_n)-t_n}{\partial w^\top x_n}\cdot \frac{\partial w^\top x_n}{\partial w_i}
$$
$$
\frac{\partial E}{\partial w_i}(w) =\sum_{n=1}^N (\sigma(w^\top x_n)-t_n) \cdot \sigma(w^\top x_n)(1-\sigma(w^\top x_n)) \cdot x_{n,i}
$$

$$
\frac{A}{B} = \frac{A}{D}\times\frac{D}{C}\times\frac{C}{B}
$$

$$
E(w) = \frac{1}{2}\sum_{n=1}^N (\sigma(w^{(1)}\cdot\sigma({w^{(0)}}^\top x_n)) - t_n)^2
$$
$$
\frac{\partial E}{\partial w^{(1)}}(w) =  \frac{\partial \frac{1}{2}\sum_{n=1}^N(\sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})) - t_n)^2}{\partial w^{(1)}}
$$
$$
\frac{\partial E}{\partial w^{(1)}}(w) = \sum_{n=1}^N \frac{\partial\frac{1}{2}(\sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})) - t_n)^2}{\partial \sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})) - t_n} \cdot \frac{\partial\sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})) - t_n}{w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})}\cdot\frac{\partial w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})}{\partial w^{(1)}}
$$
$$
\frac{\partial E}{\partial w^{(1)}}(w) = \sum_{n=1}^N (\sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n}))-t_n)\cdot \sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n}))(1-\sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})))\cdot \sigma({{w^{(0)}}^\top x_n})
$$
set
$$
z_0 = \sigma({{w^{(0)}}^\top x_n})
$$
$$
\frac{\partial E}{\partial w^{(1)}}(w) = \sum_{n=1}^N (\sigma(w^{(1)}\cdot z_0)-t_n)\cdot \sigma(w^{(1)}\cdot z_0)(1-\sigma(w^{(1)}\cdot z_0))\cdot  z_0
$$

--------------------------------------------------

$$
\frac{\partial E}{\partial w^{(0)}_i}(w) = \frac{\partial \frac{1}{2}\sum_{n=1}^N(\sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})) - t_n)^2}{\partial w^{(0)}_i}
$$
$$
\frac{\partial E}{\partial w^{(0)}_i}(w) = \sum_{n=1}^N \frac{\partial\frac{1}{2}(\sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})) - t_n)^2}{\partial \sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})) - t_n} \cdot \frac{\partial\sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})) - t_n}{w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})}\cdot\frac{\partial w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})}{\partial {w^{(0)}}^\top x_n}\cdot \frac{\partial {w^{(0)}}^\top x_n}{\partial w^{(0)}_i}
$$
$$
\frac{\partial E}{\partial w^{(0)}_i}(w) = \sum_{n=1}^N (\sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n}))-t_n)\cdot \sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n}))(1-\sigma(w^{(1)}\cdot\sigma({{w^{(0)}}^\top x_n})))\cdot w^{(1)}\sigma({{w^{(0)}}^\top x_n})(1-\sigma({{w^{(0)}}^\top x_n}))\cdot x_{n,i}
$$
$$
\frac{\partial E}{\partial w^{(0)}_i}(w) = \sum_{n=1}^N (\sigma(w^{(1)}\cdot z_0)-t_n)\cdot \sigma(w^{(1)}\cdot z_0)(1-\sigma(w^{(1)}\cdot z_0))\cdot w^{(1)} z_0(1- z_0)\cdot x_{n,i}
$$
$$
\frac{\partial E}{\partial w^{(0)}_i}(w) = 
$$
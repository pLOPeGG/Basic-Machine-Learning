\documentclass[a4paper, 10pt]{article}

\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\setlength{\marginparwidth}{2cm}

\usepackage{comment}
\usepackage{todonotes}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{enumitem}

\usepackage{xcolor}
\usepackage{graphicx}
\graphicspath{ {./img/} }

\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
    language=Python,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\title{Homework Assignment NÂ°4}
\author{BML36\\Thibault Douzon\\Rajavarman Mathivanan}
\date{September 26th, 2018}

\begin{document}
\maketitle

\pagebreak

\tableofcontents

\pagebreak
\section{Exercise 1: Decision Trees}
\subsection{Part a}
$0.51996$
\subsection{Part b}
entropy of the dataset: $0.991$
\begin{center}
    \begin{tabular}{ |c|c|c|c|c|c| }
        \hline
        Feature $a_1$ & +      & -     & $p_\text{+}$ & $p_\text{-}$ & entropy\\
        \hline
        T          & $3$    & $1$   & $\frac{3}{4}$ & $\frac{1}{4}$ & $0.811$\\
        \hline
        F          & $1$    & $4$   & $\frac{1}{5}$ & $\frac{4}{5}$ & $0.722$\\
        \hline
    \end{tabular}
\end{center}
new entropy for $a_1$: $0.811\times\frac{4}{9} + 0.722\times\frac{5}{9} = 0.762$
\\
information gain of a1 $= 0.229$

\begin{center}
    \begin{tabular}{ |c|c|c|c|c|c| }
        \hline
        Feature $a_2$ & +      & -     & $p_\text{+}$ & $p_\text{-}$ & entropy\\
        \hline
        T          & $2$    & $3$   & $\frac{2}{5}$ & $\frac{3}{5}$ & $0.971$\\
        \hline
        F          & $2$    & $2$   & $\frac{2}{4}$ & $\frac{2}{4}$ & $1$\\
        \hline
    \end{tabular}
\end{center}
new entropy for $a_2$: $0.971\times\frac{5}{9} + 1\times\frac{4}{9} = 0.762$
\\
information gain of a2 $= 0.007$

\subsection{Part c}
Entropy for 0.5 split is: 0.9910760598382223, information gain: -1.1102230246251565e-16
\\
Entropy for 1.5 split is: 0.8483857803777466, information gain: 0.14269027946047563
\\
Entropy for 2.5 split is: 0.8483857803777466, information gain: 0.14269027946047563
\\
Entropy for 3.5 split is: 0.9885107724710845, information gain: 0.002565287367137681
\\
Entropy for 4.5 split is: 0.9182958340544896, information gain: 0.07278022578373267
\\
Entropy for 5.5 split is: 0.9838614413637048, information gain: 0.007214618474517431
\\
Entropy for 6.5 split is: 0.9727652780181631, information gain: 0.018310781820059074
\\
Entropy for 7.5 split is: 0.8888888888888888, information gain: 0.10218717094933338
\\
Entropy for 8.5 split is: 0.9910760598382223, information gain: -1.1102230246251565e-16

\subsection{Part d}

best split is a1 (information gain is 0.229)

\subsection{Part e}
Error rate:
$$
\text{error} = 1 - {max}_i [p(i\vert t)]
$$
\\
a1: 
\\
error on T node: $1-3/4$
\\
error on F node: $1-4/5$
\\
global classification error on a1 split: $ (1-3/4)*4/9 + (1-4/5)*5/9 = 2/9$
\\
a2: 
\\
error on T node: $1-3/5$
\\
error on F node: $1-2/4$
\\
global classification error on a1 split: $ (1-3/5)*5/9 + (1-2/4)*4/9 = 4/9$
\\
Best split is the one with fewer global classification error -> a1


\subsection{Part f}

\end{document}

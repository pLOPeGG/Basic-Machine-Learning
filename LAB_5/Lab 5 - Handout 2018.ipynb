{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td><b>Names</b></td><td>Thibault Douzon & Rajavarman Mathivanan</td></tr>\n",
    "    <tr><td><b>Group</b></td><td>BML36</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\wx}{\\w^\\top\\x}\n",
    "\\newcommand{\\dataset}{\\mathbf{X}}\n",
    "\\newcommand{\\tset}{t\\negthickspace t}\n",
    "$\n",
    "\n",
    "# Week 5: Neural networks\n",
    "\n",
    "The basic unit of a multi-layer perceptron (MLP) is the neuron, which is a computational unit that computes a weighted sum of its inputs $\\wx$ and pushes this through a non-linear activation function. Important is that this activation should be differentiable, as we will train the weights of the network by gradient descent. This requires that we should be able to compute the gradient of an error function with respect to the weights. \n",
    "\n",
    "In a sense, this whole lab is about taking derivatives and applying the chain rule of derivation. As a reminder, the rule is that the derivative of a function $f$ applied to the result of a function $g$ $f(g(x))$ can be decomposed as:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} f(g(x)) = \\frac{\\partial f(g(x))}{\\partial g(x)} \\, \\frac{\\partial g(x)}{\\partial x} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "def gradDesc(w, errorfn, gradfn,  eta=1e-3, verbose=True, maxit=100):\n",
    "    \"\"\"Find a minimum of a function using Gradient Descent\n",
    "    \n",
    "    Inputs:\n",
    "        w: Initialisation of the weights\n",
    "        errorfn: the function of w to minimise\n",
    "        gradfn:  The gradient of that function w.r.t. w\n",
    "        eta:     The initial value of the step size\n",
    "        verbose: Whether to print out progress information\n",
    "        \n",
    "    Outputs:\n",
    "        - The value of w at which the minimum is achieved\n",
    "        - a list errors for the different iterations\n",
    "        - a list of the corresponding time steps\n",
    "    \n",
    "    \"\"\"\n",
    "    err = errorfn(w)             # Compute the error function\n",
    "    errs = np.zeros(maxit)       # Keep track of the error function\n",
    "    ts = np.zeros(maxit)         # Keep track of time stamps\n",
    "    start = time.time()\n",
    "\n",
    "    newError = errorfn(w) + 1.\n",
    "    for n in range(maxit):\n",
    "        grad = gradfn(w)\n",
    "        if verbose:\n",
    "            print(\"##\", n, \"err:\", newError, \"eta:\", eta, \"w:\", w)\n",
    "            print(\"  Gradient:\", grad)\n",
    "            print(\"  FD grad: \", gradfd(w,errorfn))\n",
    "        pw = copy.copy(w)\n",
    "        w -= grad * eta     # Update the weights\n",
    "        \n",
    "        pastError = newError\n",
    "        newError = errorfn(w)\n",
    "        while eta > 0. and pastError - newError < 0: # If the error increases, eta is too large\n",
    "            eta /= 2.0                                # halve it, and\n",
    "            w = pw - eta * grad                      # try again from the original value of the weights\n",
    "            newError = errorfn(w)\n",
    "        else:                                        # If the error goes down,\n",
    "            eta *= 1.2                               # try to increase eta a little, to speed up things.\n",
    "                \n",
    "        if pastError-newError < 1e-5:                # If we couldn't decrease the error anymore, \n",
    "            return w, errs[:n], ts[:n]               # just give up\n",
    " \n",
    "        errs[n] = newError                           # Keep track of how the errors evolved\n",
    "        ts[n] = time.time()-start\n",
    "\n",
    "    return w, errs, ts\n",
    "\n",
    "\n",
    "def sigma(a):\n",
    "    \"\"\"Numerically stable implementation of the logistic function\"\"\"\n",
    "    return 0. if a<-40. else 1. if a>40. else 1./(1.+np.exp(-a))\n",
    "sigma = np.vectorize(sigma) # make sigma work elementwise on vectors too\n",
    "\n",
    "def gradSigma(a):\n",
    "    \"\"\"Return the derivative of the logistic function w.r.t. its function parameter\"\"\"\n",
    "    s = sigma(a)\n",
    "    return s*(1.-s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron definition\n",
    "\n",
    "In this case, let's set up our network to have the sigmoidal activation function that we've seen before in logistic regression, \n",
    "$$\n",
    "\\sigma(a) = \\frac{1}{1+e^{-a}}\\ ,\n",
    "$$ \n",
    "so that the activation $z_i$ of neuron $i$ in the first computational layer is given by\n",
    "$$\n",
    "z_i = \\sigma(\\w_i^\\top \\x) \\ ,\n",
    "$$\n",
    "where $\\x$ is the input datapoint, and $\\w_i$ is the set of input weights to neuron $i$. Other activation functions are possible, such as a linear function ($z=\\wx$) or a hyperbolic tangent ($z=\\tanh(\\wx)$). Let's consider a network of a single neuron, then the output of our network $y(\\w,\\x) = \\sigma(\\wx)$.\n",
    "\n",
    "## Training\n",
    "\n",
    "To train a single neuron, we have a set of datapoints $\\dataset = \\{ \\x_1\\dots \\x_N\\}$ and corresponding targets $\\tset = \\{ t_1 \\dots t_N \\}$. We use this dataset to find optimal weights, and we define optimality on terms of an error function. Let's define our error function as a sum of squared errors:\n",
    "$$\n",
    "E(\\w) = \\tfrac{1}{2} \\sum_{n=1}^N (y(\\w,\\x_n) - t_n)^2\n",
    "$$\n",
    "\n",
    "Notice how, although our single neuron is very similar to the logistic regression we saw in previous weeks, our error function is now quite different. The error for our set $\\dataset, \\tset$ is now:\n",
    "$$\n",
    "E(\\w) = \\frac{1}{2}\\sum_{n=1}^N (\\sigma(\\w^\\top\\x_n) - t_n)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 [5 credits]** What is the partial derivative of this error with respect to the $i$th weight of the neuron, $w_i$? Be careful, this is not the same as what we saw in the lecture, where the output neuron had a linear activation function. Remember that $\\w^\\top\\x \\triangleq \\sum_{i=0}^D w_i x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSQ1\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_i}(w) = \\frac{\\partial \\frac{1}{2}\\sum_{n=1}^N (\\sigma(w^\\top x_n) - t_n)^2}{\\partial w_i}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_i}(w) =\\sum_{n=1}^N \\frac{\\partial \\frac{1}{2} (\\sigma(w^\\top x_n) - t_n)^2}{\\partial \\sigma(w^\\top x_n)-t_n} \\cdot \\frac{\\partial \\sigma(w^\\top x_n)-t_n}{\\partial w^\\top x_n}\\cdot \\frac{\\partial w^\\top x_n}{\\partial w_i}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_i}(w) =\\sum_{n=1}^N (\\sigma(w^\\top x_n)-t_n) \\cdot \\sigma(w^\\top x_n)(1-\\sigma(w^\\top x_n)) \\cdot x_{n,i}\n",
    "$$\n",
    "\n",
    "/ANSQ1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 [10 credits]** If now we had a neural network with two layers of one neuron each (one hidden and one output node), both with sigmoidal activation functions, \n",
    "* what would be the error function, analytically? \n",
    "* What is the derivative of that error with respect to the weights of the first and second layer?\n",
    "\n",
    "For ease of notation, let's assume there is no bias term in the second layer. The input is still a vector $\n",
    "\\x$ with multiple dimensions, however.\n",
    "\n",
    "How do your equations correspond to the quantities computed in backpropagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSQ2\n",
    "\n",
    "$$\n",
    "E(w) = \\frac{1}{2}\\sum_{n=1}^N (\\sigma(w^{(1)}\\cdot\\sigma({w^{(0)}}^\\top x_n)) - t_n)^2\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w^{(1)}}(w) =  \\frac{\\partial \\frac{1}{2}\\sum_{n=1}^N(\\sigma(w^{(1)}\\cdot\\sigma({{w^{(0)}}^\\top x_n})) - t_n)^2}{\\partial w^{(1)}_i}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w^{(1)}}(w) = \\sum_{n=1}^N \\frac{\\partial\\frac{1}{2}(\\sigma(w^{(1)}\\cdot\\sigma({{w^{(0)}}^\\top x_n})) - t_n)^2}{\\partial \\sigma(w^{(1)}\\cdot\\sigma({{w^{(0)}}^\\top x_n})) - t_n} \\cdot \\frac{\\partial\\sigma(w^{(1)}\\cdot\\sigma({{w^{(0)}}^\\top x_n})) - t_n}{w^{(1)}\\cdot\\sigma({{w^{(0)}}^\\top x_n})}\\cdot\\frac{\\partial w^{(1)}\\cdot\\sigma({{w^{(0)}}^\\top x_n})}{\\partial w^{(1)}}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w^{(1)}}(w) = \\sum_{n=1}^N (\\sigma(w^{(1)}\\cdot\\sigma({{w^{(0)}}^\\top x_n}))-t_n)\\cdot \\sigma(w^{(1)}\\cdot\\sigma({{w^{(0)}}^\\top x_n}))(1-\\sigma(w^{(1)}\\cdot\\sigma({{w^{(0)}}^\\top x_n})))\\cdot \\sigma({{w^{(0)}}^\\top x_n})\n",
    "$$\n",
    "set\n",
    "$$\n",
    "z_0 = \\sigma({{w^{(0)}}^\\top x_n})\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w^{(1)}}(w) = \\sum_{n=1}^N (\\sigma(w^{(1)}\\cdot z_0)-t_n)\\cdot \\sigma(w^{(1)}\\cdot z_0)(1-\\sigma(w^{(1)}\\cdot z_0))\\cdot  z_0\n",
    "$$\n",
    "/ANSQ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 [10 credits]** Implement the following python functions: \n",
    "\n",
    "* `sse(output, target)`, which implements the squared error between the given network output and the corresponding target. Both are assumed to be scalars, not vectors. \n",
    "* `gradSse(output,target)`, which computes the gradient of that error with respect to its input variable `output`\n",
    "* `neuron(weights, inputs)`, wich implements a neuron with sigmoidal activation function, weights $\\w$ and inputs $\\x$. Assume that this function will receive an augmented vector $\\x$, where the first element is fixed to $1$, so you do not need to handle the bias parameter separately\n",
    "* `gradNeuron(weights, inputs)`, which implements the gradient of the neuron's output with respect to the neuron's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANSQ3\n",
    "def sse(output, target):\n",
    "    return 0.5*(output - target)**2\n",
    "\n",
    "def gradSse(output, target):\n",
    "    return output-target\n",
    "\n",
    "def neuron(weights, inputs):\n",
    "    return sigma(weights.dot(inputs))\n",
    "\n",
    "def gradNeuron(weights, inputs):\n",
    "    return gradSigma(weights.dot(inputs))*inputs\n",
    "#/ANSQ3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 [10 credits]** Verify using finite differences that the gradients you implemented for the above functions is actually correct. To do this, we do a few runs with randomly sampled weights and inputs. In each run, compare the analytically computed gradient (the output of ``gradNeuron`` implemented above) with the result from finite differences (computed with ``neuron`` implemented above, and applying finite differences to the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD:        [0.05768946798045959, -0.046983281676293835, -0.02671729379399856, 0.04123669192779289, -0.021657034410260675]\n",
      "Analytical [ 0.05768947 -0.04698328 -0.02671729  0.04123669 -0.02165703]\n",
      " -- DIFF   [ 2.72843553e-12  9.52607437e-12  3.89673016e-12 -4.57249516e-12\n",
      "  7.23279076e-13]\n",
      "FD:        [-0.07238204419203476, -0.09614401377706915, -0.07417619883667848, 0.022760854147957362, -0.07743927178151255]\n",
      "Analytical [-0.07238204 -0.09614401 -0.0741762   0.02276085 -0.07743927]\n",
      " -- DIFF   [-2.40078790e-12 -5.98542049e-12 -3.58874042e-12 -8.73492251e-13\n",
      " -2.04253281e-12]\n",
      "FD:        [0.009833008512982744, -0.18569693989722855, -0.020734107114561695, 0.05153991597395268, -0.15094694255646246]\n",
      "Analytical [ 0.00983301 -0.18569694 -0.02073411  0.05153992 -0.15094694]\n",
      " -- DIFF   [-3.76183633e-12 -8.74889050e-12  1.06317385e-12  1.80089277e-12\n",
      " -3.28087557e-12]\n",
      "FD:        [0.04175824533381167, -0.17613609443190012, -0.049504359828089754, -0.508508686203335, -0.12776523629631775]\n",
      "Analytical [ 0.04175825 -0.17613609 -0.04950436 -0.50850869 -0.12776524]\n",
      " -- DIFF   [ 2.67055128e-12  3.96377375e-13  1.49954354e-12  1.56162860e-11\n",
      " -6.17911278e-12]\n",
      "FD:        [0.08444910358229585, 0.19792379836736093, -0.15000483600813297, 0.07228309915618958, 0.07589875143931835]\n",
      "Analytical [ 0.0844491   0.1979238  -0.15000484  0.0722831   0.07589875]\n",
      " -- DIFF   [-2.62147248e-12  2.69970157e-12  1.21588850e-12  1.78546067e-12\n",
      " -2.22018237e-12]\n",
      "FD:        [0.20511515448073944, 0.10472194667543454, -0.07077362892127237, 0.08778876044168714, -0.5022558834832935]\n",
      "Analytical [ 0.20511515  0.10472195 -0.07077363  0.08778876 -0.50225588]\n",
      " -- DIFF   [ 9.47852907e-13  1.29064814e-12  7.24295623e-13 -3.30115102e-12\n",
      "  1.54206647e-11]\n",
      "FD:        [-0.026119714535699497, -0.0113603808804541, 0.03741770399079414, 0.007249764094607868, -0.02145871146330691]\n",
      "Analytical [-0.02611971 -0.01136038  0.0374177   0.00724976 -0.02145871]\n",
      " -- DIFF   [ 3.37938011e-12  7.11930515e-12  4.00603162e-13 -3.92539386e-12\n",
      " -5.84683343e-12]\n",
      "FD:        [0.010871145861290897, 2.2656981846225218e-05, 0.0015142173015458835, 0.009429880154510073, -0.009681168378072869]\n",
      "Analytical [ 1.08711459e-02  2.26569885e-05  1.51421730e-03  9.42988015e-03\n",
      " -9.68116838e-03]\n",
      " -- DIFF   [-1.49498990e-12 -6.65496511e-12  2.02435724e-14  1.54990777e-12\n",
      "  6.33420746e-12]\n",
      "FD:        [-0.12721638641410848, -0.2609471864811752, -0.06128564237622979, -0.023060868158120137, -0.13407186546937622]\n",
      "Analytical [-0.12721639 -0.26094719 -0.06128564 -0.02306087 -0.13407187]\n",
      " -- DIFF   [ 1.61953784e-12 -4.28435065e-13 -2.23175645e-13 -5.76268200e-13\n",
      " -1.33335010e-12]\n",
      "FD:        [-0.06518725331194908, -0.03194909782844313, 0.046255610297585285, 0.037318708667744005, -0.03155702799073623]\n",
      "Analytical [-0.06518725 -0.0319491   0.04625561  0.03731871 -0.03155703]\n",
      " -- DIFF   [-5.90463789e-12 -2.13713769e-12  3.40637241e-12  7.03384573e-12\n",
      "  1.05899317e-12]\n"
     ]
    }
   ],
   "source": [
    "#ANSQ4\n",
    "\n",
    "delta = 1e-5\n",
    "for i in range(10):\n",
    "    w = np.random.randn(5)\n",
    "    x = np.random.randn(5)\n",
    "    \n",
    "    # (f(x+h)-f(x-h))/(2h)\n",
    "\n",
    "    # Your code comes here...\n",
    "    gw = [(neuron(w+delta*np.array([0 if i!=j else 1 for j in range(len(w))]), x)-neuron(w-delta*np.array([0 if i!=j else 1 for j in range(len(w))]),x))/(2*delta) for i in range(len(w))]\n",
    "    print (\"FD:       \", gw)\n",
    "    print (\"Analytical\", gradNeuron(w,x))\n",
    "    print (\" -- DIFF  \", gw-gradNeuron(w,x))\n",
    "    \n",
    "#/ANSQ4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a single neuron\n",
    "\n",
    "We now have the code to implement gradient descent on a single neuron. We simply need to apply the chain rule of derivation to get the complete gradient.\n",
    "\n",
    "**Question 5 [10 marks]** \n",
    "Implement the error function and the gradient of the error function with respect to the weights. Then use the ```gradDesc``` function (provided in the top code box) to perform gradient descent and optimise the weights based on the provided data. Plot the evolution of the error in one plot, and the function that the neuron implements overlayed with the training data (in another plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.      ]\n",
      " [0.111111]\n",
      " [0.222222]\n",
      " [0.333333]\n",
      " [0.444444]\n",
      " [0.555556]\n",
      " [0.666667]\n",
      " [0.777778]\n",
      " [0.888889]\n",
      " [1.      ]] [ 0.349486  0.830839  1.007332  0.971507  0.133066  0.166823 -0.848307\n",
      " -0.445686 -0.563567  0.261502]\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"curve.npz\")\n",
    "train = np.array([data['train'][:,0]]).T\n",
    "traint = data['train'][:,1]\n",
    "target = traint\n",
    "print(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) (2,)\n",
      "(2,) (2,)\n",
      "(2,) (2,)\n",
      "(2,) (2,)\n",
      "(2,) (2,)\n",
      "(2,) (2,)\n",
      "(2,) (2,)\n",
      "(2,) (2,)\n",
      "(2,) (2,)\n",
      "(2,) (2,)\n",
      "## 0 err: 3.3788854137065 eta: 0.001 w: [0 0]\n",
      "  Gradient: [0.78425125 0.71495191]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gradfd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-38edecfe635b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mw_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradDesc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrorfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_error_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-09173f3051ca>\u001b[0m in \u001b[0;36mgradDesc\u001b[1;34m(w, errorfn, gradfn, eta, verbose, maxit)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"##\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"err:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"eta:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"  Gradient:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"  FD grad: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradfd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merrorfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mpw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0meta\u001b[0m     \u001b[1;31m# Update the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gradfd' is not defined"
     ]
    }
   ],
   "source": [
    "#ANSQ5\n",
    "# Start by augmenting the data with a 1 so that w[0] is the bias\n",
    "X = np.ones((train.shape[0],2))\n",
    "X[:,1] = train[:,0]\n",
    "\n",
    "def error_f(weight_l):\n",
    "    input_ll, target_l = X, target\n",
    "    error = 0\n",
    "    for input_l, t in zip(input_ll, target_l):\n",
    "        error+=((neuron(weight_l, input_l))-t)**2\n",
    "    return 0.5*error\n",
    "\n",
    "\n",
    "def grad_error_f(weight_l):\n",
    "    input_ll, target_l = X, target\n",
    "    grad = np.array([0.,0.])\n",
    "    for input_l, t in zip(input_ll, target_l):\n",
    "        step =(neuron(weight_l, input_l)-t) * gradNeuron(weight_l, input_l)\n",
    "        print(step.shape, grad.shape)\n",
    "        grad += step\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "w_0 = np.array([0,0])\n",
    "w, errs, ts = gradDesc(w_0, errorfn=error_f, gradfn=grad_error_f)\n",
    "\n",
    "plt.plot(errs)\n",
    "plt.show()\n",
    "x_values = np.linspace(0,1,100)\n",
    "plt.plot(x_values, [neuron(w,i) for i in x_values])\n",
    "plt.scatter(train, target)\n",
    "\n",
    "#/ANSQ5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Defining and training a network\n",
    "\n",
    "First, let us have a look at the implementation of the error and activation functions. As you have noticed above, there is a lot of duplicated code between the functions and their gradient and, therefore, a lot of wasted computation. We can reduce this problem by implementing functions that return both the computed function value and the value of the gradient for a given input. Here are a few examples of such implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is given\n",
    "# The following functions are activation functions. For efficiency, they combine the computation of \n",
    "# the activation function itself and its gradient wrt its input values, as both often rely on computing \n",
    "# the same intermediate quantities\n",
    "\n",
    "def ssegf(y,t):\n",
    "    '''Error and gradient function for sum-of-squares'''\n",
    "    d = (y-t)\n",
    "    return .5*d**2, d\n",
    "\n",
    "def linvgf(a):\n",
    "    '''Value and gradient function for linear activation'''\n",
    "    return a,np.ones(a.shape)\n",
    "\n",
    "def logitvgf(a):\n",
    "    '''Error and gradient function for logit (sigmoidal)'''\n",
    "    s = sigma(a)\n",
    "    return s, s*(1-s)\n",
    "\n",
    "def tanh(a):\n",
    "    if a<-40:\n",
    "        return -1.\n",
    "    if a>40:\n",
    "        return 1.\n",
    "    ez = np.exp(a)\n",
    "    emz = np.exp(-a)\n",
    "    th = (ez-emz)/(ez+emz)\n",
    "    return th    \n",
    "tanh = np.vectorize(tanh)\n",
    "\n",
    "def tanhvgf(a):\n",
    "    '''Error and gradient function for hyperbolic tangent'''\n",
    "    t = tanh(a)\n",
    "    return t,1.-t*t\n",
    "\n",
    "alpha = 1e-5\n",
    "def symrelu_vgf(a):\n",
    "    if a < -5:\n",
    "        return alpha,-1+alpha*(a+5)\n",
    "    if a > 5:\n",
    "        return alpha,1+alpha*(a-5)\n",
    "    return .2,.2*a\n",
    "symreluvgf = np.vectorize(symrelu_vgf)\n",
    "\n",
    "def reluvgf(a):\n",
    "    if a<0.:\n",
    "        return 0.,0.\n",
    "    return 1.,a\n",
    "reluvgf=np.vectorize(reluvgf)\n",
    "\n",
    "x = np.linspace(-10,10,5000)\n",
    "g,v = symreluvgf(x)\n",
    "plt.plot(x,v,x,g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We define a feed-forward neural network using the following information:\n",
    "1. The number of layers in the network\n",
    "1. The number of nodes in each layer\n",
    "1. The activation function used in each layer (notice that all nodes in a layer use the same activation function)\n",
    "\n",
    "The important thing to notice in such a network, is that the input of the nodes in layer $n$ are the outputs of the nodes in layer $n-1$, so that we have a chaining of functions and the chain rule of derivatives applies directly. We can, therefore, apply the chain rule mechanically and compute the gradient using a combination of the forward-propagated inputs and backward-propagated errors.\n",
    "\n",
    "The code below implements a neural network with arbitrary numbers of hidden nodes, arbitrary numbers of hidden layers, and arbitrary activation functions. Do look at the code and make sure you understand how it works, and how to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    '''Implementation of an artificial neural network'''\n",
    "    def __init__(self, nIn, nodesPerLayer, actFn):\n",
    "        '''Initialise the network with random weights and biases. \n",
    "        Parameters are:\n",
    "        nIn: Number of input nodes (dimensionality of the data)\n",
    "        nodePerLayer: Tuple of numbers of hidden nodes, one integer per layer, including the output layer\n",
    "        actFn: Tuple of activation functions; one function per computational layer (hidden or output)'''\n",
    "        \n",
    "        assert(len(nodesPerLayer) == len(actFn))  # Check that the parameters are consistent\n",
    "        \n",
    "        self.numNodes = [nIn]                     # Create a list of numbers of nodes per layer\n",
    "        if nodesPerLayer:\n",
    "            self.numNodes.extend(nodesPerLayer)\n",
    "        self.reinit()\n",
    "        self.actfn = actFn\n",
    "        \n",
    "    def reinit(self,eta=1.0):\n",
    "        '''Re-initialise weights with random values'''\n",
    "        # Initialise weights and biases\n",
    "        self.w = [ eta * np.random.randn(self.numNodes[i],self.numNodes[i-1]) for i in range(1,len(self.numNodes)) ]\n",
    "        # print \"Weights\", self.w\n",
    "        self.biases = [ eta * np.random.randn(self.numNodes[i]) for i in range(1,len(self.numNodes))]\n",
    "        # print \"Biases:\", self.biases\n",
    "        \n",
    "    def forward(self,x):\n",
    "        ''' Compute the activations of the nodes of each layer.\n",
    "            For efficiency, use \"VGF\" functions and store the gradient of the activation function as well. In\n",
    "            other words each entry of self.fw is a tuple containing (z_i, h'(a_i))\n",
    "        '''\n",
    "        self.fw = [ self.actfn[0](self.w[0].dot(x) + self.biases[0]) ]  # First layer takes data as input\n",
    "        for h,w,b in zip(self.actfn[1:],self.w[1:],self.biases[1:]):    # following layers take the output of  \n",
    "            self.fw.append(h(w.dot(self.fw[-1][0]) + b))                #    the previous layer\n",
    "        return self.fw[-1][0]\n",
    "    \n",
    "    def back(self, errorgrad):\n",
    "        '''Implement backpropagation of the errors'''\n",
    "        self.delta = [ self.fw[-1][1] * errorgrad ]             # Start with the gradient of the error function\n",
    "        \n",
    "        for layer in range(len(self.w)-1,0,-1):\n",
    "            hprime = self.fw[layer-1][1]                        # h'(a_j)\n",
    "            delta = self.delta[0]                               # list gets extended at start, first elt: delta_k\n",
    "            w = self.w[layer]                                   # w_kj\n",
    "\n",
    "            self.delta.insert(0,hprime * (self.delta[0].dot(w))) # Use dot product to sum over all FOLLOWING weights        \n",
    "\n",
    "    def gradients(self, x, t, eta=1e-3):\n",
    "        '''Compute the gradients of the error of a single datapoint, with respect to\n",
    "        all weights and all biases'''\n",
    "        y = self.forward(x)                        # Forward propagation and computation of h'\n",
    "        error = y-t\n",
    "        self.back(error)                           # Backward propagation\n",
    "                              \n",
    "        gradw = []\n",
    "        gradb = []\n",
    "        gradw.append(np.outer(self.delta[0],x))    # derivative wrt w_ji = delta_j z_i, where z_i == x_i for input nodes\n",
    "        gradb.append(self.delta[0])                #    and z_i == 1 for bias nodes\n",
    "        \n",
    "        for i in range(1,len(self.w)):\n",
    "            gradw.append(np.outer(self.delta[i],self.fw[i-1][0])) # for folloing layers, use output of previous layer\n",
    "            gradb.append(self.delta[i])\n",
    "        return gradw,gradb,.5*error*error          # Return gradient wrt w and bias, and the error\n",
    "\n",
    "    def batchGradients(self, data,targets):\n",
    "        '''Compute the gradients of all weights and biases, for the complete dataset'''                      \n",
    "        gradw,gradb,error = self.gradients(data[0,:],targets[0])\n",
    "        for x,t in zip(data[1:,:],targets[1:]):\n",
    "            tw,tb,e = self.gradients(x,t)\n",
    "            error += e\n",
    "            for i in range(len(gradw)):\n",
    "                gradw[i] += tw[i]\n",
    "                gradb[i] += tb[i]\n",
    "        return gradw,gradb,error\n",
    "    \n",
    "    def batchStep(self,data,targets,eta=1e-3):\n",
    "        '''Make one step of batch gradient descent'''\n",
    "        gw,gb,e = self.batchGradients(data,targets)\n",
    "        for i in range(len(gw)):\n",
    "            self.w[i] -= eta*gw[i]\n",
    "            self.biases[i] -= eta*gb[i]\n",
    "        return e\n",
    "       \n",
    "    def train(self,data,targets,eta=1e-4, minit=100, maxit=10000):\n",
    "        '''Train the network using batch GD'''\n",
    "        es = np.zeros(maxit)\n",
    "        for i in range(maxit):\n",
    "            es[i] = self.batchStep(data,targets,eta)\n",
    "            if i>minit and es[i-1]< es[i]+1e-5:\n",
    "                return es[:i+1]\n",
    "        return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above code, let's create a neural network with one layer with hyperbolic tangent activation function, and one output node with linear output activation function, and train it on the provided data. We plot the evolution of the training error, as well as the function that the neural network implements (in two separate plots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f01d396e0b8>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFFhJREFUeJzt3X+MZWV9x/H3d2Z20aIpuzDdbEBcqBRDmgpkRIiGtCCIthGaEIJp2o0l2abVRtM2da1Jq0mbYJNqNWk0W0C31QqIkt0Yq25XjDGp4CDLzxV3RYi7LrujgKJtgZ359o/z3J07w8w9d37cmTmH9yuZ3PPjued89+zM59z7nPvcE5mJJKn5hla7AEnS8jDQJaklDHRJagkDXZJawkCXpJYw0CWpJQx0SWqJ2kCPiHMjYl/Xz88j4r0RsTEi9kTEgfK4YSUKliTNLRYysCgihoHDwBuAdwFPZeaNEbEd2JCZ7xtMmZKkOgsN9CuBv8vMN0bEo8BvZ+aRiNgMfCMzz+31/NNOOy23bNmypIIl6aXm3nvv/Ulmjta1G1ngdq8HPlemN2XmkTL9JLCp7slbtmxhfHx8gbuUpJe2iHiin3Z9XxSNiPXA24HPz16X1cv8OV/qR8S2iBiPiPGJiYl+dydJWqCFfMrlrcB3M/NomT9aulooj8fmelJm7sjMscwcGx2tfccgSVqkhQT6O5jubgHYDWwt01uBXctVlCRp4foK9Ig4GbgC+GLX4huBKyLiAPDmMi9JWiV9XRTNzF8Cp85a9lPg8kEUJUlaOEeKSlJLGOiS1BKNCPQ77zvEZ77d18cwJeklqxGBvnvfj7l9/EerXYYkrWmNCHRJUj0DXZJaojGBvoDvEJOkl6RGBHpErHYJkrTmNSLQJUn1DHRJagkDXZJaojGBnnN/3bokqWhEoHtJVJLqNSLQJUn1DHRJaonGBLoDiySpt0YEuuOKJKleIwJdklTPQJekljDQJaklGhPoXhSVpN76CvSIOCUi7oiI70XE/oi4JCI2RsSeiDhQHjcMrkyvikpSnX5foX8M+EpmvhZ4HbAf2A7szcxzgL1lXpK0SmoDPSJ+FbgUuBkgM5/PzGeAq4GdpdlO4JpBFSlJqtfPK/SzgAngUxFxX0TcFBEnA5sy80hp8ySwaVBFAn41lyTV6CfQR4ALgU9k5gXAL5nVvZKZyTyZGxHbImI8IsYnJiYWVaQDiySpXj+Bfgg4lJl3l/k7qAL+aERsBiiPx+Z6cmbuyMyxzBwbHR1djpolSXOoDfTMfBL4UUScWxZdDjwC7Aa2lmVbgV0DqVCS1JeRPtv9OfDZiFgPPAa8k+pkcHtE3AA8AVw3mBIr6QfRJamnvgI9M/cBY3Osunx5y5mbXeiSVK8xI0UlSb0Z6JLUEga6JLWEgS5JLdGIQHdgkSTVa0SgS5LqGeiS1BKNCXTHFUlSb40I9HBokSTVakSgS5LqGeiS1BIGuiS1RGMCPb1nkST11IhAd2CRJNVrRKBLkuoZ6JLUEo0JdAcWSVJvjQh0+9AlqV4jAl2SVM9Al6SW6Osm0RHxOPAsMAkcz8yxiNgI3AZsAR4HrsvMpwdTJn4KXZJqLOQV+u9k5vmZOVbmtwN7M/McYG+ZHwi/nEuS6i2ly+VqYGeZ3glcs/RyJEmL1W+gJ/C1iLg3IraVZZsy80iZfhLYtOzVSZL61lcfOvCmzDwcEb8G7ImI73WvzMyMiDm7ucsJYBvAmWeeuaRiJUnz6+sVemYeLo/HgDuBi4CjEbEZoDwem+e5OzJzLDPHRkdHF11oOrJIknqqDfSIODkiXtmZBq4EHgJ2A1tLs63ArkEV6TVRSarXT5fLJuDOqIZrjgD/kZlfiYjvALdHxA3AE8B1gytTklSnNtAz8zHgdXMs/ylw+SCKkiQtXGNGitqDLkm9NSLQ7UKXpHqNCHRJUj0DXZJawkCXpJZoTqB7VVSSempEoIe3LJKkWo0IdElSPQNdklqiMYFuF7ok9daIQLcHXZLqNSLQJUn1DHRJaonGBLo3uJCk3hoT6JKk3hoR6I4rkqR6jQh0SVI9A12SWqIxge4lUUnqrRGBbhe6JNVrRKBLkur1HegRMRwR90XEl8r8WRFxd0QcjIjbImL94MqUJNVZyCv09wD7u+Y/DHw0M18DPA3csJyFzea4Iknqra9Aj4gzgN8FbirzAVwG3FGa7ASuGUSBZX+D2rQktUa/r9D/GfhrYKrMnwo8k5nHy/wh4PRlrk2StAC1gR4Rvwccy8x7F7ODiNgWEeMRMT4xMbGYTUiS+tDPK/Q3Am+PiMeBW6m6Wj4GnBIRI6XNGcDhuZ6cmTsycywzx0ZHR5ehZEnSXGoDPTPfn5lnZOYW4Hrg65n5B8BdwLWl2VZg18CqBNKhRZLU01I+h/4+4C8i4iBVn/rNy1PSi3lJVJLqjdQ3mZaZ3wC+UaYfAy5a/pIkSYvhSFFJaonGBLoDiySpt2YEup3oklSrGYEuSaploEtSSzQm0O1Dl6TeGhPokqTeGhHo4VVRSarViECXJNUz0CWpJQx0SWqJRgS6NyySpHqNCHRJUj0DXZJaojGBno4skqSeGhHodqFLUr1GBLokqZ6BLkktYaBLUks0JtC9JCpJvdUGekS8LCLuiYj7I+LhiPhQWX5WRNwdEQcj4raIWD+oIh1YJEn1+nmF/hxwWWa+DjgfuCoiLgY+DHw0M18DPA3cMLgyJUl1agM9K78os+vKTwKXAXeU5TuBawZSoSSpL331oUfEcETsA44Be4AfAM9k5vHS5BBw+mBKrDiuSJJ66yvQM3MyM88HzgAuAl7b7w4iYltEjEfE+MTExKKK9AYXklRvQZ9yycxngLuAS4BTImKkrDoDODzPc3Zk5lhmjo2Oji6pWEnS/Pr5lMtoRJxSpl8OXAHspwr2a0uzrcCuQRUpSao3Ut+EzcDOiBimOgHcnplfiohHgFsj4u+B+4CbB1inJKlGbaBn5gPABXMsf4yqP31FpEOLJKmnRowUdWCRJNVrRKBLkuoZ6JLUEo0JdAcWSVJvjQh0+9AlqV4jAl2SVM9Al6SWaEyg24UuSb01JNDtRJekOg0JdElSHQNdklrCQJeklmhMoDuwSJJ6a0SgO7BIkuo1ItAlSfUMdElqiQYFup3oktRLIwLdLnRJqteIQJck1TPQJaklDHRJaonaQI+IV0XEXRHxSEQ8HBHvKcs3RsSeiDhQHjcMslAHFklSb/28Qj8O/GVmngdcDLwrIs4DtgN7M/McYG+ZHwgHFklSvdpAz8wjmfndMv0ssB84Hbga2Fma7QSuGVSRkqR6C+pDj4gtwAXA3cCmzDxSVj0JbFrWyiRJC9J3oEfEK4AvAO/NzJ93r8vMZJ6RPxGxLSLGI2J8YmJi0YXahS5JvfUV6BGxjirMP5uZXyyLj0bE5rJ+M3Bsrudm5o7MHMvMsdHR0UUVGQ4tkqRa/XzKJYCbgf2Z+ZGuVbuBrWV6K7Br+cuTJPVrpI82bwT+EHgwIvaVZX8D3AjcHhE3AE8A1w2mRElSP2oDPTO/xfxfp3L58pbTs46V2pUkNVIjRor6OXRJqteIQJck1TPQJaklDHRJaonGBLqXRCWpt0YEutdEJaleIwJdklTPQJeklmhMoDuuSJJ6a0SghyOLJKlWIwJdklTPQJekljDQJaklGhPoftuiJPXWmECXJPVmoEtSSxjoktQSjQl0e9AlqbdGBLrjiiSpXiMCXZJUrzbQI+KWiDgWEQ91LdsYEXsi4kB53DDYMiVJdfp5hf5p4KpZy7YDezPzHGBvmR8sO9ElqafaQM/MbwJPzVp8NbCzTO8ErlnmumYIb3EhSbUW24e+KTOPlOkngU3LVI8kaZGWfFE0qzH583aIRMS2iBiPiPGJiYml7k6SNI/FBvrRiNgMUB6PzdcwM3dk5lhmjo2Oji5yd5KkOosN9N3A1jK9Fdi1POXMz2uiktRbPx9b/Bzw38C5EXEoIm4AbgSuiIgDwJvL/MA4sEiS6o3UNcjMd8yz6vJlrkWStASOFJWklmhMoHuDC0nqrRGBbhe6JNVrRKBLkuoZ6JLUEga6JLVEYwLdS6KS1FsjAt2BRZJUrxGBLkmqZ6BLUks0JtAdVyRJvTUi0MNOdEmq1YhAlyTVM9AlqSUaE+jpJ9ElqadGBLo96JJUrxGBLkmqZ6BLUksY6JLUEo0I9JPWDfP88Sn+5/njq12KJK1ZtTeJXgsuOftUPr73ANu/8CCX/sYo64aD4aEgCCJgKIAyHVQDkYaCMl8tDGAo4sSyTluiLC/P697GjOmu5wMztjM0o2210Wr/cWLZiX3OsXwoKDVOb6e7VsrzhubYHqXt8ND0PiW9NC0p0CPiKuBjwDBwU2beuCxVzXLx2Rv5k0vP5qZv/ZDd9/94ELtojU7wD8f0yWZ4qJquQr/zMz0/c133c4Lhoc7JJRgu64aGZj9/7nVR6uje5kg5GY8MDTEyFAwPR/XYmR+KmY/DQ6wb6n7uXO2GGB6KEyf6zvz0vrr2Ofzi540MVXVLTReLvflyRAwD3weuAA4B3wHekZmPzPecsbGxHB8fX9T+AP73+UmOPft/HJ9KJqeSzOrz6ZkwlXni+146y6eyurl00vkumM6ymcs725gxTbVNZq2fmrW97rad6c4xPbGsa/nsfXTXSndNWWrt2t7s/U/XVbWtjkkyWeanppKpPtZ1pifL9k/MT3W2Xa3LrI571YayfPo4V+u6tzf9fzM5VW33eOdncorJMt15XE0R9BX8w10nl5Huk8s8J5t1w0MvPkmV7XfmR2bNz2g3a3/TJ7p5TkxdJ/KhcjLuvHPrPmGfaDNj/fSy6HrufO21ciLi3swcq2u3lFfoFwEHM/OxssNbgauBeQN9qV6+fphXn3ryoDavVdQ5KR2fmg7645M5PT+Zs04A87SbSiYn52h3YtnUiXUvTFYnnmrbU10nm5ntZj5O8cLkzPnjk8lzL0xxfGqyq6bZJ6wX13e8bKupZpwA4sVdgZ3I73Qxdi/sdGVW62e2nV4/80kz23XWzL0/ZrfrdF/OeO7M/c2px3lrvlXznexu2fp6zjz1V+bf4DJYSqCfDvyoa/4Q8IbZjSJiG7AN4Mwzz1zC7tRmnW6b4aHh1S5lxU3NF/xTyQuzTwyTvU9onXdEU13v3KbKu6zuddV89/rSfqp7fR/tc2b77neTQNe72enR3t2dAp13ysxq1/386XWztp0v3uZc+yO7t5NztJtbr96Ledf0OD+vHxn8Z1AGflE0M3cAO6Dqchn0/qSmGRoK1p/ow3/pndC0fJZyyjgMvKpr/oyyTJK0CpYS6N8BzomIsyJiPXA9sHt5ypIkLdSiu1wy83hEvBv4KtX7xFsy8+Flq0yStCBL6kPPzC8DX16mWiRJS9CIof+SpHoGuiS1hIEuSS1hoEtSSyz6u1wWtbOICeCJRT79NOAny1jOIFnrYDSl1qbUCdY6KMtd66szc7Su0YoG+lJExHg/X06zFljrYDSl1qbUCdY6KKtVq10uktQSBroktUSTAn3HahewANY6GE2ptSl1grUOyqrU2pg+dElSb016hS5J6qERgR4RV0XEoxFxMCK2r4F6Ho+IByNiX0SMl2UbI2JPRBwojxvK8oiIj5faH4iICwdc2y0RcSwiHupatuDaImJraX8gIrauYK0fjIjD5djui4i3da17f6n10Yh4S9fygf9+RMSrIuKuiHgkIh6OiPeU5Wvq2Paoc80d14h4WUTcExH3l1o/VJafFRF3l/3eVr7NlYg4qcwfLOu31P0bVqDWT0fED7uO6/ll+er8bWW5E8la/aH6JscfAGcD64H7gfNWuabHgdNmLftHYHuZ3g58uEy/DfhPqjtWXQzcPeDaLgUuBB5abG3ARuCx8rihTG9YoVo/CPzVHG3PK//3JwFnld+J4ZX6/QA2AxeW6VdS3U/3vLV2bHvUueaOazk2ryjT64C7y7G6Hbi+LP8k8Kdl+s+AT5bp64Hbev0bVqjWTwPXztF+Vf7/m/AK/cS9SzPzeaBz79K15mpgZ5neCVzTtfzfsvJt4JSI2DyoIjLzm8BTS6ztLcCezHwqM58G9gBXrVCt87kauDUzn8vMHwIHqX43VuT3IzOPZOZ3y/SzwH6q2zCuqWPbo875rNpxLcfmF2V2XflJ4DLgjrJ89jHtHOs7gMsjInr8G1ai1vmsyv9/EwJ9rnuX9voFXQkJfC0i7o3qnqkAmzLzSJl+EthUptdC/QutbbVrfnd5m3pLpwujR00rXmt5q38B1au0NXtsZ9UJa/C4RsRwROwDjlGF2w+AZzLz+Bz7PVFTWf8z4NTVqjUzO8f1H8px/WhEnDS71lk1DbTWJgT6WvSmzLwQeCvwroi4tHtlVu+t1uTHh9ZybcUngF8HzgeOAP+0uuXMFBGvAL4AvDczf969bi0d2znqXJPHNTMnM/N8qltYXgS8dpVLmtfsWiPiN4H3U9X8eqpulPetYomNCPQ1d+/SzDxcHo8Bd1L9Ih7tdKWUx2Ol+Vqof6G1rVrNmXm0/OFMAf/K9FvnVa81ItZRheRnM/OLZfGaO7Zz1bmWj2up7xngLuASqu6Jzs13uvd7oqay/leBn65irVeVLq7MzOeAT7HKx7UJgb6m7l0aESdHxCs708CVwEOlps4V663ArjK9G/ijctX7YuBnXW/RV8pCa/sqcGVEbChvza8sywZu1vWF36c6tp1ary+fdDgLOAe4hxX6/Sh9tTcD+zPzI12r1tSxna/OtXhcI2I0Ik4p0y8HrqDq878LuLY0m31MO8f6WuDr5V3RfP+GQdf6va6TeVD19Xcf15X/21quq6uD/KG6Yvx9qv61D6xyLWdTXVG/H3i4Uw9VX95e4ADwX8DGnL46/i+l9geBsQHX9zmqt9QvUPXP3bCY2oA/prq4dBB45wrW+u+llgeo/ig2d7X/QKn1UeCtK/n7AbyJqjvlAWBf+XnbWju2Pepcc8cV+C3gvlLTQ8Dfdv2N3VOOz+eBk8ryl5X5g2X92XX/hhWo9evluD4EfIbpT8Ksyv+/I0UlqSWa0OUiSeqDgS5JLWGgS1JLGOiS1BIGuiS1hIEuSS1hoEtSSxjoktQS/w+C+/93wrrlJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "net = NN(1,(5,1),(tanhvgf,linvgf,))\n",
    "net.reinit(eta=1.)\n",
    "errors = net.train(train,traint,1e-2,minit=100,maxit=10000)\n",
    "plt.plot(errors)\n",
    "plt.figure()\n",
    "plt.scatter(train,traint)\n",
    "x = np.arange(0,1,.01)\n",
    "y = np.array([float(net.forward([d])) for d in x])\n",
    "plt.plot(x,y)\n",
    "# print (net.w)\n",
    "# print (net.biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 [30 credits]** Use the provided test data to compare this to other configurations of your choice (more hidden layers, different activation functions, ...). Find a network that performs well on the given data. Perform multiple training runs with random initialisations, plot the final validation error for each run in function of the number of nodes, and keep the best. Plot the train and test data, as well as the function implemented by your final network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([data['test'][:,0]]).T\n",
    "testt = data['test'][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1.0688811252836732 Val:  [1.53480234] Net: [1, 1]\n",
      "Train:  1.06887876084596 Val:  [1.53478863] Net: [1, 1]\n",
      "Train:  1.0688849552985598 Val:  [1.53482451] Net: [1, 1]\n",
      "Train:  0.6383507083749744 Val:  [0.93842595] Net: [1, 1, 1]\n",
      "Train:  0.6383503638175565 Val:  [0.93842545] Net: [1, 1, 1]\n",
      "Train:  0.6383500567753834 Val:  [0.938425] Net: [1, 1, 1]\n",
      "Train:  0.3373517763352048 Val:  [0.60206712] Net: [1, 3, 1]\n",
      "Train:  0.22627408198944898 Val:  [0.4478394] Net: [1, 3, 1]\n",
      "Train:  0.21280016179335604 Val:  [0.4121321] Net: [1, 3, 1]\n",
      "Train:  0.24424973167662498 Val:  [0.48283608] Net: [1, 4, 1]\n",
      "Train:  0.23249234524774032 Val:  [0.44868429] Net: [1, 4, 1]\n",
      "Train:  0.2315741905635652 Val:  [0.43964477] Net: [1, 4, 1]\n",
      "Train:  0.22442472479055678 Val:  [0.40445917] Net: [1, 5, 1]\n",
      "Train:  0.19877584493592793 Val:  [0.3886986] Net: [1, 5, 1]\n",
      "Train:  0.215377050454244 Val:  [0.40106541] Net: [1, 5, 1]\n",
      "Train:  0.22543976544801173 Val:  [0.44871129] Net: [1, 6, 1]\n",
      "Train:  0.1908518981047056 Val:  [0.42437924] Net: [1, 6, 1]\n",
      "Train:  0.22540820545143203 Val:  [0.4469176] Net: [1, 6, 1]\n",
      "Train:  0.20264932907673944 Val:  [0.40500254] Net: [1, 7, 1]\n",
      "Train:  0.2221350150472141 Val:  [0.45003867] Net: [1, 7, 1]\n",
      "Train:  0.1988536606088876 Val:  [0.38385762] Net: [1, 7, 1]\n",
      "Train:  0.21756924271463401 Val:  [0.43410632] Net: [1, 8, 1]\n",
      "Train:  0.2149953174433632 Val:  [0.42246941] Net: [1, 8, 1]\n",
      "Train:  0.21797079363616279 Val:  [0.42762619] Net: [1, 8, 1]\n",
      "Train:  0.20479450230234209 Val:  [0.41977561] Net: [1, 10, 1]\n",
      "Train:  0.20880643864682344 Val:  [0.39117355] Net: [1, 10, 1]\n",
      "Train:  0.212998896785716 Val:  [0.4110146] Net: [1, 10, 1]\n",
      "Train:  5.085358176174038e+120 Val:  [8.54848709e+121] Net: [1, 50, 1]\n",
      "Train:  1.739744922814768e+119 Val:  [2.92451122e+120] Net: [1, 50, 1]\n",
      "Train:  9.08422287421227e+118 Val:  [1.52705787e+120] Net: [1, 50, 1]\n",
      "Train:  0.6033511145341125 Val:  [0.86969111] Net: [1, 5, 1, 1]\n",
      "Train:  0.4455201010326192 Val:  [0.60831815] Net: [1, 5, 1, 1]\n",
      "Train:  0.284292210240735 Val:  [0.54431232] Net: [1, 5, 1, 1]\n",
      "Train:  0.3331377716144855 Val:  [0.68160996] Net: [1, 5, 3, 1]\n",
      "Train:  0.3343371686697282 Val:  [0.6769984] Net: [1, 5, 3, 1]\n",
      "Train:  0.17948142669932507 Val:  [0.38409537] Net: [1, 5, 3, 1]\n",
      "Train:  0.3480936003936219 Val:  [0.67700444] Net: [1, 5, 4, 1]\n",
      "Train:  0.438995915649852 Val:  [0.60125685] Net: [1, 5, 4, 1]\n",
      "Train:  0.18163759370913593 Val:  [0.38934236] Net: [1, 5, 4, 1]\n",
      "Train:  0.1784443082035297 Val:  [0.38487417] Net: [1, 5, 5, 1]\n",
      "Train:  0.17792767044194213 Val:  [0.38533335] Net: [1, 5, 5, 1]\n",
      "Train:  0.184736286179731 Val:  [0.39179812] Net: [1, 5, 5, 1]\n",
      "Train:  0.4109429946069088 Val:  [0.79750944] Net: [1, 5, 6, 1]\n",
      "Train:  0.4781264005787994 Val:  [0.66658136] Net: [1, 5, 6, 1]\n",
      "Train:  0.18425887498579818 Val:  [0.39133776] Net: [1, 5, 6, 1]\n",
      "Train:  0.17912967765414245 Val:  [0.37382863] Net: [1, 5, 7, 1]\n",
      "Train:  0.17965520107336935 Val:  [0.37161408] Net: [1, 5, 7, 1]\n",
      "Train:  0.17969903895103304 Val:  [0.3744755] Net: [1, 5, 7, 1]\n",
      "Train:  0.333558932321868 Val:  [0.70092105] Net: [1, 10, 10, 5, 1]\n",
      "Train:  0.5900709314571498 Val:  [0.45301531] Net: [1, 10, 10, 5, 1]\n",
      "Train:  0.439900162357914 Val:  [0.59003723] Net: [1, 10, 10, 5, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1.55)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ANSQ6\n",
    "\n",
    "def evaluateNet(net):\n",
    "    err = 0.\n",
    "    for x,t in zip(test,testt):\n",
    "        err += sse(net.forward(x),t)\n",
    "    return err\n",
    "\n",
    "\n",
    "#/ANSQ6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
